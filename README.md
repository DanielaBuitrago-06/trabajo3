# Proyecto de Clasificaci√≥n de Neumon√≠a en Rayos X

Este proyecto implementa un sistema completo para la clasificaci√≥n de neumon√≠a en im√°genes de rayos X de t√≥rax utilizando descriptores cl√°sicos de forma y textura, junto con algoritmos de machine learning tradicionales y deep learning.

## üìã Contenido

1. [Descripci√≥n](#descripci√≥n)
2. [Requisitos](#requisitos)
3. [Instalaci√≥n](#instalaci√≥n)
4. [Estructura del Proyecto](#estructura-del-proyecto)
5. [Uso](#uso)
6. [Parte 1: An√°lisis y Preprocesamiento](#parte-1-an√°lisis-y-preprocesamiento)
7. [Parte 2: Extracci√≥n de Descriptores](#parte-2-extracci√≥n-de-descriptores)
8. [Parte 3: Clasificaci√≥n](#parte-3-clasificaci√≥n)
9. [Notebooks](#notebooks)
10. [Resultados](#resultados)

## üìñ Descripci√≥n

Este proyecto est√° dividido en tres partes principales:

1. **An√°lisis y Preprocesamiento**: Realiza an√°lisis exploratorio del dataset de rayos X, visualiza la distribuci√≥n de clases y dimensiones, e implementa un pipeline de preprocesamiento con normalizaci√≥n de tama√±o y ecualizaci√≥n de contraste (CLAHE).

2. **Extracci√≥n de Descriptores**: Extrae descriptores cl√°sicos de forma y textura de las im√°genes:
   - **Forma**: HOG, Momentos de Hu, Descriptores de Contorno, Descriptores de Fourier
   - **Textura**: LBP, GLCM, Filtros de Gabor, Estad√≠sticas de Primer Orden

3. **Clasificaci√≥n**: Implementa y compara m√∫ltiples algoritmos de clasificaci√≥n:
   - **M√©todos Cl√°sicos**: SVM (Linear, RBF, Polynomial), Random Forest, k-NN, Regresi√≥n Log√≠stica
   - **Deep Learning**: CNN con PyTorch

## üîß Requisitos

- Python 3.10 o superior
- pip (gestor de paquetes de Python)
- Dataset de rayos X de t√≥rax (disponible en [Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia))

## üì¶ Instalaci√≥n

### 1. Clonar o descargar el proyecto

```bash
cd trabajo3
```

### 2. Crear entorno virtual (recomendado)

```bash
python3 -m venv venv
# Mac/Linux
source venv/bin/activate  
# Windows
venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Verificar instalaci√≥n

```bash
python -c "import cv2, numpy, pandas, sklearn, torch; print('Instalaci√≥n correcta')"
```

## üìÅ Estructura del Proyecto

```
trabajo3/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ chest_xray/          # Dataset de rayos X (Kaggle)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NORMAL/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PNEUMONIA/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NORMAL/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PNEUMONIA/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ metadata.csv          # Metadatos del dataset
‚îÇ   ‚îú‚îÄ‚îÄ features_sample.csv  # Caracter√≠sticas extra√≠das
‚îÇ   ‚îî‚îÄ‚îÄ statistics.json      # Estad√≠sticas del dataset
‚îú‚îÄ‚îÄ notebooks/               # Jupyter notebooks interactivos
‚îÇ   ‚îú‚îÄ‚îÄ 1_analisis_preprocesamiento.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 2_extraccion_descriptores.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 3_calsificacion_descriptores.ipynb
‚îú‚îÄ‚îÄ results/                 # Resultados y visualizaciones
‚îÇ   ‚îú‚îÄ‚îÄ *.png                # Gr√°ficos y visualizaciones
‚îÇ   ‚îî‚îÄ‚îÄ final_comparison.csv # Comparaci√≥n de modelos
‚îú‚îÄ‚îÄ src/                     # Scripts principales
‚îÇ   ‚îú‚îÄ‚îÄ analisis_preprocesamiento.py
‚îÇ   ‚îú‚îÄ‚îÄ extraer_descriptores.py
‚îÇ   ‚îî‚îÄ‚îÄ clasificaccion_descriptores.py
‚îú‚îÄ‚îÄ venv/                    # Entorno virtual (no incluido en git)
‚îú‚îÄ‚îÄ requirements.txt         # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md               # Este archivo
```

## üöÄ Uso

### Ejecuci√≥n Secuencial (Recomendado)

Los scripts est√°n dise√±ados para ejecutarse en orden:

```bash
# 1. An√°lisis y preprocesamiento
python src/analisis_preprocesamiento.py

# 2. Extracci√≥n de descriptores
python src/extraer_descriptores.py

# 3. Clasificaci√≥n
python src/clasificaccion_descriptores.py
```

### Ejecuci√≥n Individual

Cada script puede ejecutarse de forma independiente si se cumplen los requisitos previos.

## üìä Parte 1: An√°lisis y Preprocesamiento

### Descripci√≥n

Esta parte realiza el an√°lisis exploratorio del dataset y prepara las im√°genes para el procesamiento posterior.

### Funcionalidades

- **Carga de datos**: Carga y organiza las im√°genes del dataset
- **An√°lisis exploratorio**: 
  - Distribuci√≥n de clases (NORMAL vs PNEUMONIA)
  - An√°lisis de dimensiones de im√°genes
  - Visualizaci√≥n de ejemplos
- **Preprocesamiento**:
  - Redimensionamiento a tama√±o est√°ndar (224x224)
  - Ecualizaci√≥n de contraste (CLAHE)
  - Normalizaci√≥n

### Ejecuci√≥n

```bash
python src/analisis_preprocesamiento.py
```

### Qu√© Genera

#### Archivos en `data/`:
- **`metadata.csv`**: Metadatos de todas las im√°genes (ruta, split, clase, dimensiones)
- **`statistics.json`**: Estad√≠sticas del dataset (conteos, dimensiones promedio, etc.)

#### Im√°genes en `results/`:
- **`ejemplos_imagenes.png`**: Muestras aleatorias de cada clase
- **`distribucion_clases.png`**: Gr√°ficos de distribuci√≥n de clases
- **`analisis_dimensiones.png`**: An√°lisis de dimensiones de im√°genes
- **`comparacion_redimensionamiento.png`**: Comparaci√≥n de m√©todos de redimensionamiento
- **`comparacion_ecualizacion.png`**: Comparaci√≥n de m√©todos de ecualizaci√≥n
- **`pipeline_preprocesamiento.png`**: Visualizaci√≥n del pipeline completo

### Ejemplo de Salida

```
‚úÖ Directorio encontrado: ../data/chest_xray
üìä Distribuci√≥n de datos:
  Train - Normal: 1341
  Train - Pneumonia: 3875
  Test - Normal: 234
  Test - Pneumonia: 390

‚úÖ Dataset creado: 5840 im√°genes totales

üìä Estad√≠sticas de distribuci√≥n:
class  NORMAL  PNEUMONIA
split                   
test      234        390
train    1341       3875

Total im√°genes: 5840
Balance de clases: 26.97% NORMAL vs 73.03% PNEUMONIA
```

## üîç Parte 2: Extracci√≥n de Descriptores

### Descripci√≥n

Extrae descriptores cl√°sicos de forma y textura de las im√°genes preprocesadas.

### Descriptores Implementados

#### Descriptores de Forma:
1. **HOG (Histogram of Oriented Gradients)**: Captura la distribuci√≥n de gradientes locales
2. **Momentos de Hu**: 7 momentos invariantes a traslaci√≥n, rotaci√≥n y escala
3. **Descriptores de Contorno**: √Årea, per√≠metro, circularidad, excentricidad, solidez
4. **Descriptores de Fourier**: Representaci√≥n del contorno en el dominio de la frecuencia

#### Descriptores de Textura:
1. **LBP (Local Binary Patterns)**: Patrones binarios locales para textura
2. **GLCM (Gray Level Co-occurrence Matrix)**: Matriz de co-ocurrencia de niveles de gris
3. **Filtros de Gabor**: Respuestas a diferentes frecuencias y orientaciones
4. **Estad√≠sticas de Primer Orden**: Media, varianza, asimetr√≠a, curtosis, entrop√≠a

### Ejecuci√≥n

```bash
python src/extraer_descriptores.py
```

### Qu√© Genera

#### Archivos en `data/`:
- **`features_sample.csv`**: Dataset con todas las caracter√≠sticas extra√≠das (26,338 caracter√≠sticas por imagen)

#### Im√°genes en `results/`:
- **`hog_visualization.png`**: Visualizaci√≥n de caracter√≠sticas HOG
- **`hu_moments.png`**: Visualizaci√≥n de momentos de Hu
- **`contour_features.png`**: Contornos detectados
- **`fourier_descriptors.png`**: Descriptores de Fourier
- **`lbp_features.png`**: Visualizaci√≥n de LBP
- **`glcm_features.png`**: Matriz GLCM
- **`gabor_features.png`**: Respuestas de filtros de Gabor
- **`first_order_stats.png`**: Estad√≠sticas de primer orden

### Ejemplo de Salida

```
‚úÖ HOG extra√≠do: 26244 caracter√≠sticas
‚úÖ Momentos de Hu calculados: 7 caracter√≠sticas
‚úÖ Descriptores de contorno extra√≠dos: 5 caracter√≠sticas
‚úÖ Descriptores de Fourier extra√≠dos: 20 coeficientes
‚úÖ LBP extra√≠do: 26 caracter√≠sticas
‚úÖ Caracter√≠sticas GLCM extra√≠das: 6 propiedades
‚úÖ Caracter√≠sticas de Gabor extra√≠das: 24 caracter√≠sticas
‚úÖ Estad√≠sticas de primer orden extra√≠das: 6 estad√≠sticas

Total: 26338 caracter√≠sticas
```

## ü§ñ Parte 3: Clasificaci√≥n

### Descripci√≥n

Implementa y compara m√∫ltiples algoritmos de clasificaci√≥n usando los descriptores extra√≠dos.

### Algoritmos Implementados

1. **SVM (Support Vector Machine)**:
   - Kernel Linear
   - Kernel RBF
   - Kernel Polynomial

2. **Random Forest**: Clasificador basado en √°rboles de decisi√≥n

3. **k-NN (k-Nearest Neighbors)**: Clasificador basado en vecinos m√°s cercanos

4. **Regresi√≥n Log√≠stica**: Modelo lineal probabil√≠stico

5. **CNN (Convolutional Neural Network)**: Red neuronal convolucional con PyTorch

### Ejecuci√≥n

```bash
python src/clasificaccion_descriptores.py
```

### Qu√© Genera

#### Archivos en `results/`:
- **`pca_analysis.png`**: An√°lisis de componentes principales
- **`cm_*.png`**: Matrices de confusi√≥n para cada modelo
- **`model_comparison.png`**: Comparaci√≥n visual de modelos
- **`roc_curves.png`**: Curvas ROC de todos los modelos
- **`rf_importance.png`**: Importancia de caracter√≠sticas (Random Forest)
- **`final_comparison.csv`**: Tabla comparativa de todos los modelos
- **`final_comparison.png`**: Visualizaci√≥n final de comparaci√≥n

### M√©tricas Evaluadas

- **Accuracy**: Precisi√≥n general
- **Precision**: Precisi√≥n por clase
- **Recall**: Sensibilidad por clase
- **F1-Score**: Media arm√≥nica de precisi√≥n y recall
- **ROC AUC**: √Årea bajo la curva ROC
- **Validaci√≥n Cruzada**: 5-fold cross-validation

### Ejemplo de Salida

```
üìä Comparaci√≥n de Modelos:
         Classifier  Accuracy  Precision  Recall  F1-Score  CV Mean   CV Std  ROC AUC
         SVM Linear      0.80   0.842105    0.80  0.745098   0.8625 0.082916 0.973333
            SVM RBF      0.75   0.562500    0.75  0.642857   0.7750 0.030619 0.920000
     SVM Polynomial      0.75   0.562500    0.75  0.642857   0.7750 0.030619 0.093333
      Random Forest      0.75   0.562500    0.75  0.642857   0.7625 0.025000 0.940000
         k-NN (k=3)      0.80   0.842105    0.80  0.745098   0.8250 0.082916 0.693333
Logistic Regression      0.80   0.842105    0.80  0.745098   0.8750 0.079057 0.986667
      CNN (PyTorch)      0.65   0.422500    0.65  0.512121   0.7802      NaN 0.780220
```

## üìì Notebooks

Los notebooks de Jupyter proporcionan una versi√≥n interactiva de cada script, ideal para experimentaci√≥n y an√°lisis detallado.

### Ejecutar Notebooks

```bash
# Desde el directorio del proyecto
jupyter notebook notebooks/

# O abrir directamente
jupyter notebook notebooks/1_analisis_preprocesamiento.ipynb
```

### Ventajas de los Notebooks

- Ejecuci√≥n celda por celda
- Visualizaci√≥n interactiva de resultados
- F√°cil modificaci√≥n de par√°metros
- An√°lisis paso a paso
- Documentaci√≥n integrada

## üìà Resultados

### Estructura de Resultados

Todos los resultados se guardan autom√°ticamente en:

- **`results/`**: Im√°genes generadas, gr√°ficos y comparaciones
- **`data/`**: Datasets procesados y caracter√≠sticas extra√≠das

### Archivos de Salida

Cada ejecuci√≥n genera:

1. **Archivos CSV**: Datasets con caracter√≠sticas y resultados
2. **Archivos JSON**: Estad√≠sticas y metadatos estructurados
3. **Im√°genes PNG**: Visualizaciones y gr√°ficos de an√°lisis

### Interpretaci√≥n de Resultados

#### Parte 1 (Preprocesamiento)
- **Distribuci√≥n de clases**: Balance del dataset (t√≠picamente desbalanceado en datasets m√©dicos)
- **Dimensiones**: Variabilidad en tama√±os de im√°genes (requiere normalizaci√≥n)
- **Preprocesamiento**: Mejora del contraste y normalizaci√≥n para mejor extracci√≥n de caracter√≠sticas

#### Parte 2 (Extracci√≥n)
- **N√∫mero de caracter√≠sticas**: Total de descriptores extra√≠dos (26,338 en este proyecto)
- **Visualizaciones**: Permiten entender qu√© capturan los descriptores
- **Tiempo de procesamiento**: Depende del tama√±o del dataset

#### Parte 3 (Clasificaci√≥n)
- **Accuracy > 0.75**: Buen rendimiento para dataset m√©dico
- **ROC AUC > 0.85**: Excelente capacidad de discriminaci√≥n
- **F1-Score**: Balance entre precisi√≥n y recall (importante en datasets desbalanceados)
- **Validaci√≥n cruzada**: Confiabilidad del modelo

## üîç Soluci√≥n de Problemas

### Error: "No se encontr√≥ metadata.csv"
- **Soluci√≥n**: Ejecuta primero `analisis_preprocesamiento.py`

### Error: "No se encontr√≥ features_sample.csv"
- **Soluci√≥n**: Ejecuta primero `extraer_descriptores.py`

### Error: "Dataset no encontrado"
- **Soluci√≥n**: Descarga el dataset de [Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia) y col√≥calo en `data/chest_xray/`

### Memoria insuficiente al extraer caracter√≠sticas
- **Soluci√≥n**: Usa `sample_size` en `extract_features_batch()` para procesar una muestra m√°s peque√±a

### Modelos con bajo rendimiento
- **Soluciones**:
  - Aumenta el tama√±o de la muestra
  - Ajusta hiperpar√°metros de los modelos
  - Considera usar PCA para reducci√≥n de dimensionalidad
  - Prueba diferentes combinaciones de descriptores

## üìù Notas T√©cnicas

### Algoritmos y T√©cnicas Utilizadas

- **CLAHE (Contrast Limited Adaptive Histogram Equalization)**: Ecualizaci√≥n adaptativa de histograma, ideal para radiograf√≠as
- **HOG**: Descriptor robusto para detecci√≥n de objetos
- **Momentos de Hu**: Invariantes a transformaciones geom√©tricas
- **LBP**: Descriptor eficiente para textura
- **GLCM**: An√°lisis de textura basado en estad√≠sticas de segundo orden
- **PCA (Principal Component Analysis)**: Reducci√≥n de dimensionalidad
- **RANSAC**: Eliminaci√≥n de outliers en matching de caracter√≠sticas
- **Cross-Validation**: Validaci√≥n robusta de modelos

### Par√°metros Ajustables

En cada script puedes modificar:

- **Parte 1**: Tama√±o de redimensionamiento, par√°metros CLAHE
- **Parte 2**: Par√°metros de cada descriptor (orientaciones HOG, radio LBP, etc.)
- **Parte 3**: Hiperpar√°metros de modelos, n√∫mero de componentes PCA, √©pocas de CNN

### Descriptores y sus Invarianzas

- **Momentos de Hu**: Invariantes a traslaci√≥n, rotaci√≥n y escala
- **Descriptores de Fourier**: Invariantes a rotaci√≥n (solo magnitud)
- **HOG**: Parcialmente invariante a iluminaci√≥n
- **LBP**: Invariante a cambios de iluminaci√≥n monot√≥nicos

## üìä Dataset

### Informaci√≥n del Dataset

- **Fuente**: [Kaggle - Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)
- **Total de im√°genes**: 5,840
- **Clases**: 
  - NORMAL: 1,575 im√°genes (27%)
  - PNEUMONIA: 4,265 im√°genes (73%)
- **Divisi√≥n**:
  - Train: 5,216 im√°genes
  - Test: 624 im√°genes
  - Val: 16 im√°genes

### Caracter√≠sticas del Dataset

- **Formato**: JPEG
- **Dimensiones**: Variables (promedio ~970x1327 p√≠xeles)
- **Balance**: Desbalanceado (m√°s casos de neumon√≠a)
- **Calidad**: Im√°genes m√©dicas reales con variabilidad en calidad y orientaci√≥n

## üìÑ Licencia

Este proyecto es parte de un trabajo acad√©mico de la Universidad Nacional.

## üë• Autor

Daniela Buitrago, estudiante de la Universidad Nacional.

---

**√öltima actualizaci√≥n**: 2025

Para m√°s informaci√≥n sobre los descriptores y algoritmos, consulta la documentaci√≥n en cada m√≥dulo.
